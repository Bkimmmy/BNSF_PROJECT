{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Sequence Models: LSTM vs GRU Bake-off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Deep sequence models: LSTM vs GRU bake-off\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 130\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Spark -> Pandas & features\n",
    "# -----------------------------\n",
    "# Use the same filtered dataset (no leakage after last failure)\n",
    "train_df = df_final.filter(df_final[\"RUL_minutes\"].isNotNull())\n",
    "pdf = train_df.toPandas()\n",
    "\n",
    "# Add cyclic time-of-day\n",
    "if \"timestamp_bin\" in pdf.columns:\n",
    "    tod_min = pdf[\"timestamp_bin\"].dt.hour * 60 + pdf[\"timestamp_bin\"].dt.minute\n",
    "    pdf[\"tod_sin\"] = np.sin(2*np.pi * tod_min / 1440.0)\n",
    "    pdf[\"tod_cos\"] = np.cos(2*np.pi * tod_min / 1440.0)\n",
    "\n",
    "drop_cols = [\n",
    "    \"timestamp_bin\", \"failure\", \"next_failure_time\",\n",
    "    \"last_failure_time\", \"minutes_since_last_failure\", \"RUL_minutes\"\n",
    "]\n",
    "y = pdf[\"RUL_minutes\"].astype(float).to_numpy()\n",
    "\n",
    "feat_cols = [c for c in pdf.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(pdf[c])]\n",
    "X = pdf[feat_cols].replace([np.inf, -np.inf], np.nan).astype(float)\n",
    "\n",
    "# time-ordered split identical to classic models\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=False)\n",
    "train_meds = X_train.median(numeric_only=True)\n",
    "X_train = X_train.fillna(train_meds)\n",
    "X_test  = X_test.fillna(train_meds)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Build sequences (same cut)\n",
    "# -----------------------------\n",
    "X_full = pd.concat([X_train, X_test], axis=0)[feat_cols].to_numpy(dtype=np.float32)\n",
    "y_full = np.concatenate([y_train, y_test]).astype(np.float32)\n",
    "cut = len(y_train)  # first index of the classical TEST set\n",
    "\n",
    "SEQ_LEN = 30  # ~1 hour if your bins are 2 min; change if needed\n",
    "\n",
    "Xs, ys, tgt_idx = [], [], []\n",
    "for i in range(len(X_full) - SEQ_LEN):\n",
    "    Xs.append(X_full[i:i+SEQ_LEN])\n",
    "    ys.append(y_full[i+SEQ_LEN])\n",
    "    tgt_idx.append(i + SEQ_LEN)\n",
    "Xs = np.asarray(Xs, dtype=np.float32)\n",
    "ys = np.asarray(ys, dtype=np.float32)\n",
    "tgt_idx = np.asarray(tgt_idx)\n",
    "\n",
    "# Split sequences so that the target belongs to train/test by the same time cut\n",
    "mask_test = tgt_idx >= cut\n",
    "Xseq_tr, Yseq_tr = Xs[~mask_test], ys[~mask_test]\n",
    "Xseq_te, Yseq_te = Xs[mask_test],  ys[mask_test]\n",
    "\n",
    "# Normalize using TRAIN only\n",
    "mu  = Xseq_tr.mean(axis=(0,1), keepdims=True)\n",
    "std = Xseq_tr.std(axis=(0,1), keepdims=True); std[std == 0] = 1e-6\n",
    "Xseq_tr = (Xseq_tr - mu) / std\n",
    "Xseq_te = (Xseq_te - mu) / std\n",
    "\n",
    "# Create a small validation tail from training (time-ordered, no shuffle)\n",
    "val_frac = 0.1\n",
    "n_tr = len(Xseq_tr)\n",
    "n_val = int(n_tr * val_frac)\n",
    "Xseq_val, Yseq_val = Xseq_tr[-n_val:], Yseq_tr[-n_val:]\n",
    "Xseq_tr,  Yseq_tr  = Xseq_tr[:-n_val], Yseq_tr[:-n_val]\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2) Bin-weighted loss to balance long vs short RUL\n",
    "# ---------------------------------------------------\n",
    "# Define bins and compute inverse-frequency weights on TRAIN targets only\n",
    "bin_edges = np.array([0, 5, 10, 20, 40, 80, 160, 1e9], dtype=np.float32)\n",
    "tr_bins = np.digitize(Yseq_tr, bin_edges) - 1\n",
    "counts = np.bincount(np.clip(tr_bins, 0, len(bin_edges)-2), minlength=len(bin_edges)-1)\n",
    "# class-balanced weighting: total/(#bins * count)\n",
    "class_w = (len(Yseq_tr) / ((len(bin_edges)-1) * np.maximum(counts, 1))).astype(np.float32)\n",
    "# gentle cap to avoid extreme weights\n",
    "class_w = np.clip(class_w, 0.3, 5.0)\n",
    "\n",
    "def make_sample_weights(y):\n",
    "    b = np.digitize(y, bin_edges) - 1\n",
    "    b = np.clip(b, 0, len(class_w)-1)\n",
    "    return class_w[b]\n",
    "\n",
    "w_tr = make_sample_weights(Yseq_tr)\n",
    "w_val = make_sample_weights(Yseq_val)\n",
    "w_te  = make_sample_weights(Yseq_te)  # not used in loss, just for analysis if needed\n",
    "\n",
    "# ----------------------\n",
    "# 3) PyTorch data pipes\n",
    "# ----------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def to_loader(X, y, w, batch=256, shuffle=False):\n",
    "    X_t = torch.tensor(X, dtype=torch.float32)\n",
    "    y_t = torch.tensor(y, dtype=torch.float32)\n",
    "    w_t = torch.tensor(w, dtype=torch.float32)\n",
    "    ds = TensorDataset(X_t, y_t, w_t)\n",
    "    return DataLoader(ds, batch_size=batch, shuffle=shuffle, pin_memory=True)\n",
    "\n",
    "train_ld = to_loader(Xseq_tr,  Yseq_tr,  w_tr, shuffle=True)\n",
    "val_ld   = to_loader(Xseq_val, Yseq_val, w_val, shuffle=False)\n",
    "test_ld  = to_loader(Xseq_te,  Yseq_te,  w_te, shuffle=False)\n",
    "\n",
    "n_features = Xseq_tr.shape[2]\n",
    "\n",
    "# ----------------------\n",
    "# 4) Model definitions\n",
    "# ----------------------\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64, layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features, hidden_size=hidden,\n",
    "            num_layers=layers, batch_first=True, dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :]).squeeze(1)\n",
    "\n",
    "class GRURegressor(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64, layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=n_features, hidden_size=hidden,\n",
    "            num_layers=layers, batch_first=True, dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        return self.fc(out[:, -1, :]).squeeze(1)\n",
    "\n",
    "# weighted MSE\n",
    "def weighted_mse(pred, target, weight):\n",
    "    return torch.mean((pred - target) ** 2 * weight)\n",
    "\n",
    "# ----------------------\n",
    "# 5) Train/eval routine\n",
    "# ----------------------\n",
    "def r2_score_np(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    ss_tot = np.sum((y_true - y_true.mean())**2) + 1e-12\n",
    "    return 1.0 - ss_res/ss_tot\n",
    "\n",
    "def within_tol(y_true, y_pred, tol):\n",
    "    return 100.0 * np.mean(np.abs(y_pred - y_true) <= tol)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_model(model, loader):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(device)\n",
    "        yhat = model(xb).cpu().numpy()\n",
    "        preds.append(yhat)\n",
    "        trues.append(yb.numpy())\n",
    "    return np.concatenate(trues), np.concatenate(preds)\n",
    "\n",
    "def train_one(model, epochs=50, lr=1e-3, patience=6, clip=1.0):\n",
    "    model = model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    best_state, best_val = None, float(\"inf\")\n",
    "    train_hist, val_hist = [], []\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        # ---- train\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for xb, yb, wb in train_ld:\n",
    "            xb, yb, wb = xb.to(device), yb.to(device), wb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            pred = model(xb)\n",
    "            loss = weighted_mse(pred, yb, wb)\n",
    "            loss.backward()\n",
    "            if clip is not None:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            opt.step()\n",
    "            running += loss.item()\n",
    "        train_loss = running / max(1, len(train_ld))\n",
    "        train_hist.append(train_loss)\n",
    "\n",
    "        # ---- validate\n",
    "        model.eval()\n",
    "        v_running = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, wb in val_ld:\n",
    "                xb, yb, wb = xb.to(device), yb.to(device), wb.to(device)\n",
    "                pred = model(xb)\n",
    "                v_running += weighted_mse(pred, yb, wb).item()\n",
    "        val_loss = v_running / max(1, len(val_ld))\n",
    "        val_hist.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {ep:02d} | train {train_loss:.4f}  val {val_loss:.4f}\")\n",
    "\n",
    "        # early stopping\n",
    "        if val_loss + 1e-6 < best_val:\n",
    "            best_val = val_loss\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {ep} (best val {best_val:.4f})\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model, train_hist, val_hist\n",
    "\n",
    "def evaluate(name, model):\n",
    "    yt, yp = predict_model(model, test_ld)\n",
    "    out = {\n",
    "        \"Model\": name,\n",
    "        \"MAE (min)\": mean_absolute_error(yt, yp),\n",
    "        \"RMSE (min)\": mean_squared_error(yt, yp, squared=False),\n",
    "        \"R²\": r2_score_np(yt, yp),\n",
    "        \"≤5 min %\": within_tol(yt, yp, 5),\n",
    "        \"≤10 min %\": within_tol(yt, yp, 10),\n",
    "        \"Bias (min)\": float(np.mean(yp - yt)),\n",
    "        \"N\": len(yt)\n",
    "    }\n",
    "    return out, yt, yp\n",
    "\n",
    "# ----------------------\n",
    "# 6) Run small grid\n",
    "# ----------------------\n",
    "configs = [\n",
    "    (\"LSTM\", 32, 2), (\"LSTM\", 64, 2), (\"LSTM\", 128, 2),\n",
    "    (\"GRU\",  32, 2), (\"GRU\",  64, 2), (\"GRU\",  128, 2),\n",
    "]\n",
    "\n",
    "results = []\n",
    "pred_cache = {}   # name -> (yt, yp)\n",
    "for kind, hidden, layers in configs:\n",
    "    print(f\"\\n=== Training {kind} hidden={hidden} layers={layers} ===\")\n",
    "    if kind.lower() == \"lstm\":\n",
    "        net = LSTMRegressor(n_features, hidden=hidden, layers=layers, dropout=0.2)\n",
    "    else:\n",
    "        net = GRURegressor(n_features, hidden=hidden, layers=layers, dropout=0.2)\n",
    "\n",
    "    net, tr_hist, va_hist = train_one(net, epochs=50, lr=1e-3, patience=6, clip=1.0)\n",
    "    name = f\"{kind}-{hidden}x{layers}\"\n",
    "    res, yt, yp = evaluate(name, net)\n",
    "    results.append(res)\n",
    "    pred_cache[name] = (yt, yp)\n",
    "\n",
    "# ----------------------\n",
    "# 7) Show results\n",
    "# ----------------------\n",
    "metrics_df = pd.DataFrame(results).sort_values(\"RMSE (min)\").reset_index(drop=True)\n",
    "display(metrics_df.style.format({\n",
    "    \"MAE (min)\": \"{:.2f}\", \"RMSE (min)\": \"{:.2f}\", \"R²\": \"{:.3f}\",\n",
    "    \"≤5 min %\": \"{:.1f}\", \"≤10 min %\": \"{:.1f}\", \"Bias (min)\": \"{:.2f}\"\n",
    "}))\n",
    "\n",
    "best_name = metrics_df.iloc[0][\"Model\"]\n",
    "yt_best, yp_best = pred_cache[best_name]\n",
    "\n",
    "# Quick visual: best deep model Pred vs Actual + error histogram\n",
    "fig, ax = plt.subplots(1, 2, figsize=(13,5))\n",
    "# Pred vs Actual\n",
    "mn, mx = float(np.percentile(yt_best, 1)), float(np.percentile(yt_best, 99))\n",
    "pad = 0.05*(mx - mn + 1e-6)\n",
    "ax[0].scatter(yt_best, yp_best, s=10, alpha=0.5)\n",
    "ax[0].plot([mn-pad, mx+pad], [mn-pad, mx+pad], \"k--\", lw=1)\n",
    "ax[0].set_title(f\"{best_name}: Pred vs Actual\")\n",
    "ax[0].set_xlabel(\"Actual RUL (min)\"); ax[0].set_ylabel(\"Predicted RUL (min)\")\n",
    "ax[0].grid(True, alpha=0.3)\n",
    "# Error histogram\n",
    "err = yp_best - yt_best\n",
    "ax[1].hist(err, bins=60, alpha=0.8)\n",
    "ax[1].axvline(0, color=\"k\", lw=1)\n",
    "ax[1].axvline(err.mean(), color=\"tab:red\", ls=\"--\", lw=1, label=f\"bias={err.mean():.2f}\")\n",
    "ax[1].set_title(f\"{best_name}: Error (Pred-Actual)\")\n",
    "ax[1].set_xlabel(\"Error (min)\"); ax[1].set_ylabel(\"Count\"); ax[1].legend()\n",
    "ax[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
