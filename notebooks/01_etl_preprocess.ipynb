{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load In Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read IoT CSV directly from S3\n",
    "iot_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://hqpsusu-ml-data-bucket/raw/iot/metropt.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# Preview data\n",
    "iot_df.show(5)\n",
    "iot_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning – Timestamps and GPS Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, round as spark_round\n",
    "\n",
    "# Data Cleaning Step\n",
    "# 1. Convert `timestamp` column from string to Spark's native timestamp type\n",
    "#    → Ensures proper handling of time-series features (sorting, grouping, windowing).\n",
    "# 2. Drop rows with missing GPS latitude/longitude values\n",
    "#    → Removes incomplete records that would break geospatial analysis.\n",
    "# 3. Add `lat_rounded` and `lon_rounded` columns (rounded to 2 decimals)\n",
    "#    → Simplifies location-based grouping and aggregation (e.g., hourly metrics per station/area).\n",
    "iot_clean = (\n",
    "    iot_df\n",
    "    .withColumn(\"timestamp\", to_timestamp(\"timestamp\"))   \n",
    "    .dropna(subset=[\"gpsLat\", \"gpsLong\"])                \n",
    "    .withColumn(\"lat_rounded\", spark_round(col(\"gpsLat\"), 2))\n",
    "    .withColumn(\"lon_rounded\", spark_round(col(\"gpsLong\"), 2))\n",
    ")\n",
    "\n",
    "# Preview cleaned data to confirm changes\n",
    "iot_clean.show(5)\n",
    "iot_clean.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering – Create Hourly Time Buckets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, hour, date_trunc\n",
    "\n",
    "# Feature Engineering Step\n",
    "# 1. Create `timestamp_hour` by truncating the full timestamp to the nearest hour.\n",
    "#    → This allows us to aggregate and analyze data at an hourly level \n",
    "#      (reduces noise and makes trends clearer).\n",
    "# 2. Retain `lat_rounded` and `lon_rounded` from the cleaning step\n",
    "#    → So later we can group by both time and location.\n",
    "iot_features = (\n",
    "    iot_clean\n",
    "    .withColumn(\"timestamp_hour\", date_trunc(\"hour\", col(\"timestamp\")))\n",
    ")\n",
    "\n",
    "# Quick sample to validate hourly truncation\n",
    "iot_features.select(\"timestamp\", \"timestamp_hour\", \"lat_rounded\", \"lon_rounded\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering – Hourly Aggregations by Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, stddev, min, max\n",
    "\n",
    "# Feature Engineering Step\n",
    "# Aggregate IoT sensor values at the (hour, location) level.\n",
    "# Why:\n",
    "#   - Reduces granularity → smooths out second-level noise\n",
    "#   - Captures meaningful hourly trends for each rail segment\n",
    "#   - Generates summary statistics useful for predictive modeling\n",
    "#\n",
    "# Features created:\n",
    "#   • Mean + standard deviation of pressure/temperature sensors\n",
    "#   • Min, max, and average motor current (useful for anomaly detection)\n",
    "#   • Hourly-level averages for TP2, TP3, H1, and DV pressure\n",
    "iot_hourly = (\n",
    "    iot_features\n",
    "    .groupBy(\"timestamp_hour\", \"lat_rounded\", \"lon_rounded\")\n",
    "    .agg(\n",
    "        avg(\"TP2\").alias(\"TP2_avg\"),\n",
    "        stddev(\"TP2\").alias(\"TP2_std\"),\n",
    "        avg(\"TP3\").alias(\"TP3_avg\"),\n",
    "        avg(\"H1\").alias(\"H1_avg\"),\n",
    "        avg(\"DV_pressure\").alias(\"DV_pressure_avg\"),\n",
    "        avg(\"Oil_temperature\").alias(\"Oil_temp_avg\"),\n",
    "        avg(\"Motor_current\").alias(\"Motor_current_avg\"),\n",
    "        max(\"Motor_current\").alias(\"Motor_current_max\"),\n",
    "        min(\"Motor_current\").alias(\"Motor_current_min\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Quick sanity check – inspect aggregated output\n",
    "iot_hourly.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist Processed Features to Storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define output storage path (AWS S3 bucket for processed features)\n",
    "output_path = \"s3://hqpsusu-ml-data-bucket/processed/iot_hourly_features\"\n",
    "\n",
    "# Save the aggregated hourly features in Parquet format\n",
    "# Why Parquet:\n",
    "#   - Columnar format → efficient for analytics and ML pipelines\n",
    "#   - Supports schema evolution and compression\n",
    "#   - Ideal for large-scale Spark workloads\n",
    "iot_hourly.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"✅ Features saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Validate Processed Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load processed features back from S3 (saved in Parquet format)\n",
    "features = spark.read.parquet(\"s3://hqpsusu-ml-data-bucket/processed/iot_hourly_features\")\n",
    "\n",
    "# Inspect schema to confirm expected columns and types\n",
    "features.printSchema()\n",
    "\n",
    "# Show a sample of processed records\n",
    "features.show(5)\n",
    "\n",
    "# Generate quick summary statistics on key features\n",
    "# Helps verify ranges, distributions, and catch anomalies\n",
    "features.describe([\"TP2_avg\", \"TP3_avg\", \"Oil_temp_avg\", \"Motor_current_avg\"]).show()\n",
    "\n",
    "# Visualize trends (Databricks built-in display function)\n",
    "# Example: Track motor current over time\n",
    "display(features.select(\"timestamp_hour\", \"Motor_current_avg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Distribution of Categorical Signals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check the distribution of categorical / binary sensor signals\n",
    "# These represent switches, flags, or on/off states (e.g., COMP, LPS, MPG).\n",
    "# Grouping and counting helps identify class imbalance, rare categories, \n",
    "# or useless features (e.g., signals always stuck at 0).\n",
    "\n",
    "for col in [\"COMP\", \"LPS\", \"MPG\", \"Pressure_switch\", \"DV_eletric\", \"Towers\", \"Oil_level\"]:\n",
    "    iot_raw.groupBy(col).count().orderBy(col).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Failure Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Add binary failure column based on strict failure rule:\n",
    "# A failure is flagged when COMP = 0, MPG = 0, and LPS = 0 simultaneously.\n",
    "iot_labeled = (\n",
    "    iot_raw.withColumn(\n",
    "        \"failure\",\n",
    "        F.when(\n",
    "            (F.col(\"COMP\") == 0) &\n",
    "            (F.col(\"MPG\") == 0) &\n",
    "            (F.col(\"LPS\") == 0),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Preview some rows with the new failure label\n",
    "iot_labeled.select(\"timestamp\", \"COMP\", \"MPG\", \"LPS\", \"failure\").show(10)\n",
    "\n",
    "# Count distribution of failure vs. non-failure records\n",
    "iot_labeled.groupBy(\"failure\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Features with Failure Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Define failure flag in the raw IoT dataset:\n",
    "# A failure occurs when COMP = 0, MPG = 0, and LPS = 0 simultaneously.\n",
    "iot_failures = (\n",
    "    iot_raw\n",
    "    .withColumn(\n",
    "        \"failure\",\n",
    "        F.when(\n",
    "            (F.col(\"COMP\") == 0) & \n",
    "            (F.col(\"MPG\") == 0) & \n",
    "            (F.col(\"LPS\") == 0),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    # Add spatial + temporal resolution for later joins\n",
    "    .withColumn(\"timestamp_hour\", F.date_trunc(\"hour\", F.col(\"timestamp\")))\n",
    "    .withColumn(\"lat_rounded\", F.round(F.col(\"gpsLat\"), 2))\n",
    "    .withColumn(\"lon_rounded\", F.round(F.col(\"gpsLong\"), 2))\n",
    ")\n",
    "\n",
    "# 2. Aggregate failures at the hourly and location level\n",
    "failures_hourly = (\n",
    "    iot_failures\n",
    "    .groupBy(\"timestamp_hour\", \"lat_rounded\", \"lon_rounded\")\n",
    "    .agg(F.max(\"failure\").alias(\"failure\"))   # if any failure in hour, mark = 1\n",
    ")\n",
    "\n",
    "# 3. Load previously engineered hourly features\n",
    "features = spark.read.parquet(\"s3://hqpsusu-ml-data-bucket/processed/iot_hourly_features\")\n",
    "\n",
    "# 4. Join features with failure labels on timestamp + location\n",
    "features_with_failures = (\n",
    "    features.join(\n",
    "        failures_hourly,\n",
    "        on=[\"timestamp_hour\", \"lat_rounded\", \"lon_rounded\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .fillna({\"failure\": 0})   # ensure missing = no failure\n",
    ")\n",
    "\n",
    "# 5. Quick validation: check class balance + preview sample\n",
    "features_with_failures.groupBy(\"failure\").count().show()\n",
    "features_with_failures.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Failure Labeling (Row-Level → Hourly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Reload previously engineered features\n",
    "features = spark.read.parquet(\"s3://hqpsusu-ml-data-bucket/processed/iot_hourly_features\")\n",
    "\n",
    "# 2. Reload raw IoT data (needed for re-labeling)\n",
    "iot_raw = (\n",
    "    spark.read.csv(\n",
    "        \"s3://hqpsusu-ml-data-bucket/raw/iot/metropt.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3. Define a stricter failure rule at the row level\n",
    "# Rule: failure = 1 if COMP=0, MPG=0, and LPS=0 simultaneously\n",
    "# Then aggregate to hourly resolution (failure=1 if ANY row in the hour has failure)\n",
    "iot_fail = (\n",
    "    iot_raw\n",
    "    .withColumn(\n",
    "        \"failure\", \n",
    "        F.when(\n",
    "            (F.col(\"COMP\") == 0) & \n",
    "            (F.col(\"MPG\") == 0) & \n",
    "            (F.col(\"LPS\") == 0),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    .withColumn(\"timestamp_hour\", F.date_trunc(\"hour\", F.col(\"timestamp\")))\n",
    "    .groupBy(\"timestamp_hour\")\n",
    "    .agg(F.max(\"failure\").alias(\"failure\"))\n",
    ")\n",
    "\n",
    "# 4. Join the stricter failure labels onto the engineered features\n",
    "features_with_failure = features.join(iot_fail, on=\"timestamp_hour\", how=\"left\")\n",
    "\n",
    "# 5. Preview results\n",
    "features_with_failure.select(\n",
    "    \"timestamp_hour\", \n",
    "    \"Motor_current_avg\", \n",
    "    \"Oil_temp_avg\", \n",
    "    \"DV_pressure_avg\", \n",
    "    \"failure\"\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Failure Labeling – Compute Remaining Useful Life (RUL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "\n",
    "# Define window ordered by time\n",
    "w = Window.orderBy(\"timestamp_hour\") \\\n",
    "           .rowsBetween(0, Window.unboundedFollowing)\n",
    "\n",
    "# Step 1. Find the next failure timestamp for each row\n",
    "features = features.withColumn(\n",
    "    \"next_failure_time\",\n",
    "    F.min(F.when(F.col(\"failure\") == 1, F.col(\"timestamp_hour\"))).over(w)\n",
    ")\n",
    "\n",
    "# Step 2. Compute Remaining Useful Life (RUL) in hours\n",
    "features = features.withColumn(\n",
    "    \"RUL_hours\",\n",
    "    F.when(\n",
    "        F.col(\"next_failure_time\").isNotNull(),\n",
    "        (F.unix_timestamp(\"next_failure_time\") - F.unix_timestamp(\"timestamp_hour\")) / 3600\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 3. Force failures to have RUL = 0\n",
    "features = features.withColumn(\n",
    "    \"RUL_hours\",\n",
    "    F.when(F.col(\"failure\") == 1, F.lit(0.0)).otherwise(F.col(\"RUL_hours\"))\n",
    ")\n",
    "\n",
    "# Inspect\n",
    "features.select(\"timestamp_hour\", \"failure\", \"next_failure_time\", \"RUL_hours\").show(30, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Optional Final Feature Engineering – Binning, Rolling Stats, and RUL Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "\n",
    "# -----------------------------------\n",
    "# 1) Load raw IoT data and define failure\n",
    "# -----------------------------------\n",
    "raw_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://hqpsusu-ml-data-bucket/raw/iot/metropt.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define a raw failure condition (COMP=0 & MPG=0 & LPS=0)\n",
    "raw_df = raw_df.withColumn(\n",
    "    \"failure_raw\",\n",
    "    F.when((F.col(\"COMP\") == 0) & (F.col(\"MPG\") == 0) & (F.col(\"LPS\") == 0), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# -----------------------------------\n",
    "# 2) Bin into 2-minute intervals\n",
    "#    - This reduces granularity and smooths sensor signals\n",
    "# -----------------------------------\n",
    "raw_df = raw_df.withColumn(\n",
    "    \"timestamp_bin\",\n",
    "    (F.col(\"timestamp\").cast(\"long\") / 120).cast(\"long\") * 120\n",
    ")\n",
    "raw_df = raw_df.withColumn(\"timestamp_bin\", F.from_unixtime(\"timestamp_bin\").cast(\"timestamp\"))\n",
    "\n",
    "# -----------------------------------\n",
    "# 3) Aggregate engineered features\n",
    "#    - Numeric signals → avg, std, min, max\n",
    "#    - Binary/state signals → max (presence indicator)\n",
    "# -----------------------------------\n",
    "numeric_cols = [\n",
    "    \"Motor_current\", \"Oil_temperature\", \"DV_pressure\",\n",
    "    \"TP2\", \"TP3\", \"H1\", \"Reservoirs\", \"Flowmeter\",\n",
    "    \"Caudal_impulses\", \"gpsSpeed\"\n",
    "]\n",
    "\n",
    "binary_cols = [\n",
    "    \"COMP\", \"MPG\", \"LPS\", \"Pressure_switch\",\n",
    "    \"DV_eletric\", \"Towers\", \"Oil_level\", \"gpsQuality\"\n",
    "]\n",
    "\n",
    "agg_exprs = []\n",
    "for c in numeric_cols:\n",
    "    agg_exprs += [\n",
    "        F.avg(c).alias(f\"{c}_avg\"),\n",
    "        F.stddev(c).alias(f\"{c}_std\"),\n",
    "        F.min(c).alias(f\"{c}_min\"),\n",
    "        F.max(c).alias(f\"{c}_max\"),\n",
    "    ]\n",
    "for c in binary_cols:\n",
    "    agg_exprs += [F.max(F.col(c).cast(\"int\")).alias(f\"{c}_flag\")]\n",
    "\n",
    "# Carry the failure flag to each bin\n",
    "agg_exprs += [F.max(\"failure_raw\").alias(\"failure\")]\n",
    "\n",
    "features = (\n",
    "    raw_df.groupBy(\"timestamp_bin\")\n",
    "          .agg(*agg_exprs)\n",
    "          .orderBy(\"timestamp_bin\")\n",
    ")\n",
    "\n",
    "# -----------------------------------\n",
    "# 4) Rolling statistics, lags, deltas\n",
    "#    - 30 min window (15 bins of 2 min each)\n",
    "# -----------------------------------\n",
    "w_order = Window.orderBy(\"timestamp_bin\")\n",
    "w_roll_30m = w_order.rowsBetween(-15, -1)\n",
    "\n",
    "key_bases = [\n",
    "    \"Motor_current_avg\",\n",
    "    \"Oil_temperature_avg\",\n",
    "    \"DV_pressure_avg\"\n",
    "]\n",
    "\n",
    "for base in key_bases:\n",
    "    features = features.withColumn(f\"{base}_roll_mean_30m\", F.avg(F.col(base)).over(w_roll_30m))\n",
    "    features = features.withColumn(f\"{base}_roll_std_30m\",  F.stddev(F.col(base)).over(w_roll_30m))\n",
    "    features = features.withColumn(f\"{base}_lag1\",          F.lag(F.col(base), 1).over(w_order))\n",
    "    features = features.withColumn(f\"{base}_delta\",         F.col(base) - F.col(f\"{base}_lag1\"))\n",
    "\n",
    "# -----------------------------------\n",
    "# 5) Remaining Useful Life (RUL) calculation\n",
    "#    - Next failure time\n",
    "#    - Last failure time\n",
    "#    - RUL in minutes\n",
    "# -----------------------------------\n",
    "w_fwd  = Window.orderBy(\"timestamp_bin\").rowsBetween(0, Window.unboundedFollowing)\n",
    "w_back = Window.orderBy(\"timestamp_bin\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "features = features.withColumn(\n",
    "    \"next_failure_time\",\n",
    "    F.first(F.when(F.col(\"failure\") == 1, F.col(\"timestamp_bin\")), ignorenulls=True).over(w_fwd)\n",
    ")\n",
    "\n",
    "features = features.withColumn(\n",
    "    \"last_failure_time\",\n",
    "    F.last(F.when(F.col(\"failure\") == 1, F.col(\"timestamp_bin\")), ignorenulls=True).over(w_back)\n",
    ")\n",
    "\n",
    "features = features.withColumn(\n",
    "    \"RUL_minutes\",\n",
    "    F.when(F.col(\"failure\") == 1, F.lit(0.0))\n",
    "     .otherwise((F.unix_timestamp(\"next_failure_time\") - F.unix_timestamp(\"timestamp_bin\")) / 60.0)\n",
    ")\n",
    "\n",
    "features = features.withColumn(\n",
    "    \"minutes_since_last_failure\",\n",
    "    (F.unix_timestamp(\"timestamp_bin\") - F.unix_timestamp(\"last_failure_time\")) / 60.0\n",
    ")\n",
    "\n",
    "# Optional: fill NA roll stats at the beginning\n",
    "fill_zero_cols = [f\"{b}_roll_mean_30m\" for b in key_bases] + \\\n",
    "                 [f\"{b}_roll_std_30m\" for b in key_bases] + \\\n",
    "                 [f\"{b}_lag1\" for b in key_bases] + \\\n",
    "                 [f\"{b}_delta\" for b in key_bases]\n",
    "features = features.fillna(0, subset=fill_zero_cols)\n",
    "\n",
    "# -----------------------------------\n",
    "# 6) Final dataset\n",
    "# -----------------------------------\n",
    "df_final = features\n",
    "\n",
    "# -----------------------------------\n",
    "# 7) Preview schema and dimensions\n",
    "# -----------------------------------\n",
    "print(\"Schema:\")\n",
    "df_final.printSchema()\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "df_final.show(10, truncate=False)\n",
    "\n",
    "print(\"\\nRow count:\", df_final.count())\n",
    "print(\"Column count:\", len(df_final.columns))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
