{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison: RandomForest vs XGBoost vs LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ========= RF vs XGB vs LSTM (sequence) — unified comparison =========\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from io import BytesIO\n",
    "import boto3, joblib, types\n",
    "\n",
    "# ------------------------------\n",
    "# 0) Load RF/XGB from S3 (joblib)\n",
    "# ------------------------------\n",
    "bucket = \"hqpsusu-ml-data-bucket\"\n",
    "rf_key  = \"final_project/models/rf_model.pkl\"\n",
    "xgb_key = \"final_project/models/xgb_model.pkl\"\n",
    "\n",
    "def load_joblib_from_s3(bucket, key):\n",
    "    buf = BytesIO()\n",
    "    boto3.client(\"s3\").download_fileobj(bucket, key, buf)\n",
    "    buf.seek(0)\n",
    "    return joblib.load(buf)\n",
    "\n",
    "rf_loaded  = load_joblib_from_s3(bucket, rf_key)\n",
    "xgb_loaded = load_joblib_from_s3(bucket, xgb_key)\n",
    "\n",
    "# --------------------------------------\n",
    "# 1) Produce predictions for each model\n",
    "# --------------------------------------\n",
    "models = {}\n",
    "\n",
    "# A) Tabular models (same X_test / y_test)\n",
    "y_true_tab = np.asarray(y_test)  # from your sklearn split\n",
    "y_pred_rf  = rf_loaded.predict(X_test)\n",
    "y_pred_xgb = xgb_loaded.predict(X_test)\n",
    "models[\"RandomForest (S3)\"] = (y_true_tab, y_pred_rf)\n",
    "models[\"XGBoost (S3)\"]      = (y_true_tab, y_pred_xgb)\n",
    "\n",
    "# B) LSTM (sequence) — optional, only if the vars exist\n",
    "try:\n",
    "    import torch\n",
    "    have = all(v in globals() for v in [\"model\", \"X_test_t\"])\n",
    "    if have:\n",
    "        # y_test for LSTM could be numpy (from your code) or torch tensor\n",
    "        if \"y_test\" in globals() and isinstance(globals()[\"y_test\"], (np.ndarray, list, pd.Series)):\n",
    "            y_true_lstm = np.asarray(globals()[\"y_test\"])\n",
    "        elif \"y_test_t\" in globals():\n",
    "            y_true_lstm = globals()[\"y_test_t\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            y_true_lstm = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred_lstm = model(X_test_t).detach().cpu().numpy()\n",
    "\n",
    "        if y_true_lstm is not None and len(y_true_lstm) == len(y_pred_lstm):\n",
    "            models[\"LSTM (seq)\"] = (y_true_lstm, y_pred_lstm)\n",
    "        else:\n",
    "            print(\"ℹ️ LSTM found but y_test shapes not aligned — skipping LSTM in comparison.\")\n",
    "    else:\n",
    "        print(\"ℹ️ LSTM variables not found — skipping LSTM in comparison.\")\n",
    "except Exception as e:\n",
    "    print(f\"ℹ️ Skipping LSTM due to: {e}\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 2) Metrics\n",
    "# --------------------------------------\n",
    "def metrics(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    mask   = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    yt, yp = y_true[mask], y_pred[mask]\n",
    "    err, ae = (yp - yt), np.abs(yp - yt)\n",
    "    eps = 1e-9\n",
    "    return {\n",
    "        \"MAE\": mean_absolute_error(yt, yp),\n",
    "        \"RMSE\": mean_squared_error(yt, yp, squared=False),\n",
    "        \"R²\": r2_score(yt, yp),\n",
    "        \"Bias\": float(err.mean()),\n",
    "        \"sMAPE %\": 100.0 * np.mean(2*ae / (np.abs(yt)+np.abs(yp)+eps)),\n",
    "        \"≤5 min %\": 100.0 * np.mean(ae <= 5),\n",
    "        \"≤10 min %\": 100.0 * np.mean(ae <= 10),\n",
    "        \"N\": int(yt.size),\n",
    "    }\n",
    "\n",
    "summary_rows = []\n",
    "for name, (yt, yp) in models.items():\n",
    "    m = metrics(yt, yp); m[\"Model\"] = name\n",
    "    summary_rows.append(m)\n",
    "results = pd.DataFrame(summary_rows).set_index(\"Model\").round(3).sort_values(\"MAE\")\n",
    "print(results)\n",
    "\n",
    "# --------------------------------------\n",
    "# 3) Helpers for plots\n",
    "# --------------------------------------\n",
    "def binned_xy(y_true, y_pred, nbins=40):\n",
    "    df = pd.DataFrame({\"y\": y_true, \"yp\": y_pred}).dropna()\n",
    "    q = min(nbins, max(2, int(df.shape[0]**0.5)))\n",
    "    df[\"bin\"] = pd.qcut(df[\"y\"], q=q, duplicates=\"drop\")\n",
    "    g = df.groupby(\"bin\").agg(y_mean=(\"y\",\"mean\"), yp_mean=(\"yp\",\"mean\"))\n",
    "    return g[\"y_mean\"].values, g[\"yp_mean\"].values\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 140\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "grid = fig.add_gridspec(2, 3, hspace=0.35, wspace=0.3)\n",
    "\n",
    "colors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\"]\n",
    "names  = list(models.keys())\n",
    "\n",
    "# 3A) Calibration (binned)\n",
    "ax1 = fig.add_subplot(grid[0,0])\n",
    "mn = min(np.min(v[0]) for v in models.values())\n",
    "mx = max(np.max(v[0]) for v in models.values())\n",
    "ax1.plot([mn, mx], [mn, mx], \"k--\", lw=1, label=\"Ideal\")\n",
    "for (name, (yt, yp)), c in zip(models.items(), colors):\n",
    "    xb, yb = binned_xy(yt, yp)\n",
    "    ax1.plot(xb, yb, marker=\"o\", ms=3, lw=1.8, label=name, color=c)\n",
    "ax1.set_title(\"Calibration (binned): Predicted vs Actual\")\n",
    "ax1.set_xlabel(\"Actual RUL (min)\"); ax1.set_ylabel(\"Predicted RUL (min)\")\n",
    "ax1.legend(); ax1.grid(alpha=0.25)\n",
    "\n",
    "# 3B) Accuracy vs Tolerance (CDF style)\n",
    "ax2 = fig.add_subplot(grid[0,1])\n",
    "tol = np.arange(0, 61, 1)\n",
    "for (name, (yt, yp)), c in zip(models.items(), colors):\n",
    "    ae = np.abs(yp - yt)\n",
    "    acc = [(ae <= t).mean()*100.0 for t in tol]\n",
    "    ax2.plot(tol, acc, lw=2, label=name, color=c)\n",
    "ax2.set_title(\"Accuracy vs Tolerance (higher is better)\")\n",
    "ax2.set_xlabel(\"Tolerance (± minutes)\"); ax2.set_ylabel(\"Within-tolerance (%)\")\n",
    "ax2.set_ylim(0, 100); ax2.legend(); ax2.grid(alpha=0.25)\n",
    "\n",
    "# 3C) Absolute error distributions (boxplot)\n",
    "ax3 = fig.add_subplot(grid[0,2])\n",
    "box_data = [np.abs(models[n][1] - models[n][0]) for n in names]\n",
    "ax3.boxplot(box_data, labels=names, showmeans=True)\n",
    "ax3.set_title(\"Absolute Error Distribution\"); ax3.set_ylabel(\"|Error| (min)\")\n",
    "ax3.grid(axis=\"y\", alpha=0.25)\n",
    "\n",
    "# 3D) Residuals vs Actual\n",
    "ax4 = fig.add_subplot(grid[1,0])\n",
    "for (name, (yt, yp)), c in zip(models.items(), colors):\n",
    "    ax4.scatter(yt, (yp - yt), s=6, alpha=0.22, label=name, color=c)\n",
    "ax4.axhline(0, color=\"k\", ls=\"--\", lw=1)\n",
    "ax4.set_title(\"Residuals vs Actual\"); ax4.set_xlabel(\"Actual RUL (min)\"); ax4.set_ylabel(\"Pred - Actual (min)\")\n",
    "ax4.legend(); ax4.grid(alpha=0.25)\n",
    "\n",
    "# 3E) MAE by Actual-RUL bins\n",
    "ax5 = fig.add_subplot(grid[1,1])\n",
    "bins = [0, 5, 10, 20, 40, 80, 160, 9999]\n",
    "labels = [\"0–5\",\"5–10\",\"10–20\",\"20–40\",\"40–80\",\"80–160\",\">160\"]\n",
    "x = np.arange(len(labels)); w = 0.8 / max(1, len(names))\n",
    "for i, ((name, (yt, yp)), c) in enumerate(zip(models.items(), colors)):\n",
    "    idx = np.digitize(yt, bins, right=True) - 1\n",
    "    dfb = pd.DataFrame({\"bin\": idx, \"ae\": np.abs(yp - yt)})\n",
    "    mae_bins = dfb.groupby(\"bin\")[\"ae\"].mean().reindex(range(len(labels))).values\n",
    "    ax5.bar(x + (i - (len(names)-1)/2)*w, mae_bins, width=w, label=name, color=c)\n",
    "ax5.set_xticks(x, labels); ax5.set_ylabel(\"MAE (min)\")\n",
    "ax5.set_title(\"MAE by Actual-RUL Bin (lower is better)\")\n",
    "ax5.legend(); ax5.grid(axis=\"y\", alpha=0.25)\n",
    "\n",
    "# 3F) Metrics table\n",
    "ax6 = fig.add_subplot(grid[1,2]); ax6.axis(\"off\")\n",
    "tbl = ax6.table(cellText=results.reset_index().values,\n",
    "                colLabels=[\"Model\"] + list(results.columns),\n",
    "                cellLoc=\"center\", loc=\"center\")\n",
    "tbl.auto_set_font_size(False); tbl.set_fontsize(9)\n",
    "ax6.set_title(\"Summary Metrics\", pad=8)\n",
    "\n",
    "plt.suptitle(\"Model Comparison: RF vs XGBoost vs LSTM\", fontsize=16, y=0.98)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Comparison (Presentation-Ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Presentation-ready comparison\n",
    "# ============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ---- Helper: register any (name, y_true, y_pred) that truly align ----\n",
    "pairs = []\n",
    "\n",
    "def add_pair(name, y_true, y_pred):\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    if len(y_true) != len(y_pred):\n",
    "        print(f\"⚠️ Skipping {name}: length mismatch y_true={len(y_true)} vs y_pred={len(y_pred)}\")\n",
    "        return\n",
    "    if not np.isfinite(y_true).all() or not np.isfinite(y_pred).all():\n",
    "        mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "        print(f\"ℹ️ {name}: dropping {np.size(mask)-mask.sum()} non-finite rows\")\n",
    "        y_true, y_pred = y_true[mask], y_pred[mask]\n",
    "    pairs.append({\"name\": name, \"y_true\": y_true, \"y_pred\": y_pred})\n",
    "\n",
    "# ---- Try to collect models that may exist in your notebook ----\n",
    "# Classical models (RF/XGB) usually share the same y_test_tab/y_test_classic. Adjust names if needed.\n",
    "if \"y_pred_rf\" in locals() and \"y_test\" in locals():\n",
    "    add_pair(\"Random Forest\", y_test, y_pred_rf)\n",
    "if \"y_pred_xgb\" in locals() and \"y_test\" in locals():\n",
    "    add_pair(\"XGBoost\", y_test, y_pred_xgb)\n",
    "\n",
    "# LSTM: if you used the snippet where LSTM reused 'y_test' and 'y_pred',\n",
    "# we try both common possibilities:\n",
    "if \"y_pred_lstm\" in locals() and \"y_test_lstm\" in locals():\n",
    "    add_pair(\"LSTM\", y_test_lstm, y_pred_lstm)\n",
    "elif \"y_pred\" in locals() and \"y_test\" in locals():\n",
    "    add_pair(\"LSTM\", y_test, y_pred)\n",
    "\n",
    "if not pairs:\n",
    "    raise RuntimeError(\"No valid (y_true, y_pred) pairs found. Make sure prediction variables are in memory.\")\n",
    "\n",
    "# ---- Metrics table ----\n",
    "def smape(y_true, y_pred, eps=1e-6):\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps)\n",
    "    return 100.0 * np.mean(np.abs(y_pred - y_true) / denom)\n",
    "\n",
    "rows = []\n",
    "for p in pairs:\n",
    "    yt, yp = p[\"y_true\"], p[\"y_pred\"]\n",
    "    mae  = mean_absolute_error(yt, yp)\n",
    "    rmse = mean_squared_error(yt, yp, squared=False)\n",
    "    bias = float(np.mean(yp - yt))\n",
    "    s_mape = smape(yt, yp)\n",
    "    rows.append({\"Model\": p[\"name\"], \"MAE (min)\": mae, \"RMSE (min)\": rmse, \"Bias (min)\": bias, \"sMAPE (%)\": s_mape})\n",
    "\n",
    "metrics_df = pd.DataFrame(rows).sort_values(\"RMSE (min)\").reset_index(drop=True)\n",
    "print(\"✅ Summary metrics (lower is better):\")\n",
    "display(metrics_df.style.format({\"MAE (min)\": \"{:.2f}\", \"RMSE (min)\": \"{:.2f}\", \"Bias (min)\": \"{:.2f}\", \"sMAPE (%)\": \"{:.1f}\"}))\n",
    "\n",
    "# ---- Global ranges for comparable axes ----\n",
    "all_true = np.concatenate([p[\"y_true\"] for p in pairs])\n",
    "low, high = np.percentile(all_true, [1, 99])\n",
    "pad = 0.05 * (high - low + 1e-6)\n",
    "xy_min, xy_max = low - pad, high + pad\n",
    "\n",
    "# Error range (clip for readability, but still show full histogram tails via bins)\n",
    "all_err = np.concatenate([p[\"y_pred\"] - p[\"y_true\"] for p in pairs])\n",
    "e_low, e_high = np.percentile(all_err, [1, 99])\n",
    "e_pad = 0.05 * (e_high - e_low + 1e-6)\n",
    "err_min, err_max = e_low - e_pad, e_high + e_pad\n",
    "\n",
    "# ---- Figure 1: Per-model Predicted vs Actual ----\n",
    "cols = len(pairs)\n",
    "fig, axes = plt.subplots(1, cols, figsize=(6*cols, 5))\n",
    "axes = np.atleast_1d(axes)\n",
    "\n",
    "for ax, p in zip(axes, pairs):\n",
    "    yt, yp, name = p[\"y_true\"], p[\"y_pred\"], p[\"name\"]\n",
    "    # subsample for readability if huge\n",
    "    n = len(yt)\n",
    "    idx = np.linspace(0, n-1, min(n, 300)).astype(int)\n",
    "    ax.scatter(yt[idx], yp[idx], alpha=0.45, s=20, edgecolor=\"k\", linewidths=0.2)\n",
    "    ax.plot([xy_min, xy_max], [xy_min, xy_max], \"r--\", lw=1.5, label=\"Ideal = Predicted\")\n",
    "    ax.set_xlim(xy_min, xy_max)\n",
    "    ax.set_ylim(xy_min, xy_max)\n",
    "    ax.set_title(f\"{name}: Predicted vs Actual\")\n",
    "    ax.set_xlabel(\"Actual RUL (minutes)\")\n",
    "    ax.set_ylabel(\"Predicted RUL (minutes)\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- Figure 2: Overlaid Error Histogram (all models) ----\n",
    "plt.figure(figsize=(7,5))\n",
    "bins = 60\n",
    "for p in pairs:\n",
    "    err = p[\"y_pred\"] - p[\"y_true\"]\n",
    "    plt.hist(np.clip(err, err_min, err_max), bins=bins, alpha=0.45, label=p[\"name\"])\n",
    "# vertical line at 0 and per-model mean bias\n",
    "plt.axvline(0, color=\"k\", lw=1)\n",
    "for p in pairs:\n",
    "    mu = float(np.mean(p[\"y_pred\"] - p[\"y_true\"]))\n",
    "    plt.axvline(mu, linestyle=\"--\", lw=1, label=f\"{p['name']} mean={mu:.1f}\")\n",
    "plt.xlim(err_min, err_max)\n",
    "plt.title(\"Prediction Error Distribution (Pred - Actual)\")\n",
    "plt.xlabel(\"Error (minutes)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- Figure 3: Time-slice (first N samples) ----\n",
    "N = 200\n",
    "fig, axes = plt.subplots(len(pairs), 1, figsize=(12, 3.2*len(pairs)), sharex=True)\n",
    "axes = np.atleast_1d(axes)\n",
    "for ax, p in zip(axes, pairs):\n",
    "    yt, yp = p[\"y_true\"][:N], p[\"y_pred\"][:N]\n",
    "    ax.plot(yt, marker=\"o\", ms=3, lw=1, label=\"Actual RUL\")\n",
    "    ax.plot(yp, marker=\"x\", ms=3, lw=1, label=\"Predicted RUL\")\n",
    "    ax.set_title(f\"{p['name']}: First {N} samples\")\n",
    "    ax.set_ylabel(\"RUL (min)\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "axes[-1].set_xlabel(\"Sample Index\")\n",
    "axes[0].legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- Optional: LSTM training loss curve if you tracked epoch_losses ----\n",
    "if \"epoch_losses\" in locals() and len(epoch_losses) > 0:\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(range(1, len(epoch_losses)+1), epoch_losses, marker=\"o\")\n",
    "    plt.title(\"LSTM Training Loss (per epoch)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ℹ️ No LSTM epoch loss history found. To record it, append the mean batch loss each epoch to `epoch_losses` during training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Model Comparison: RF vs XGB vs LSTM (Same Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Full Model Comparison: RF vs XGB vs LSTM\n",
    "# ============================================================\n",
    "# This notebook section:\n",
    "# 1) Uses the same features and temporal split for ALL models.\n",
    "# 2) Trains Random Forest, XGBoost, and LSTM.\n",
    "# 3) Evaluates with multiple metrics (MAE, RMSE, R², Bias, sMAPE, tolerance%).\n",
    "# 4) Produces visual comparisons: scatter, histograms, tolerance curves, residuals, bins, time slices.\n",
    "# 5) Includes LSTM training loss for transparency.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 130\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 0) Spark → Pandas & feature matrix used by ALL models\n",
    "# -------------------------------------------------------\n",
    "# Keep only rows with a defined RUL (no leakage from \"after last failure\")\n",
    "train_df = df_final.filter(df_final[\"RUL_minutes\"].isNotNull())\n",
    "pdf = train_df.toPandas()\n",
    "\n",
    "# Cyclic time-of-day (optional but helpful and consistent across models)\n",
    "if \"timestamp_bin\" in pdf.columns:\n",
    "    tod_min = pdf[\"timestamp_bin\"].dt.hour * 60 + pdf[\"timestamp_bin\"].dt.minute\n",
    "    pdf[\"tod_sin\"] = np.sin(2*np.pi * tod_min / 1440.0)\n",
    "    pdf[\"tod_cos\"] = np.cos(2*np.pi * tod_min / 1440.0)\n",
    "\n",
    "# Drop non-features / leakage columns\n",
    "drop_cols = [\n",
    "    \"timestamp_bin\", \"failure\", \"next_failure_time\",\n",
    "    \"last_failure_time\", \"minutes_since_last_failure\", \"RUL_minutes\"\n",
    "]\n",
    "y = pdf[\"RUL_minutes\"].astype(float).to_numpy()\n",
    "\n",
    "# numeric features only, exclude leakage\n",
    "feat_cols = [c for c in pdf.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(pdf[c])]\n",
    "X = pdf[feat_cols].replace([np.inf, -np.inf], np.nan).astype(float)\n",
    "\n",
    "# time-ordered split (same for all models)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=False)\n",
    "\n",
    "# Impute with TRAIN medians (no leakage)\n",
    "train_meds = X_train.median(numeric_only=True)\n",
    "X_train = X_train.fillna(train_meds)\n",
    "X_test  = X_test.fillna(train_meds)\n",
    "\n",
    "print(f\"Features: {len(feat_cols)} | Train {X_train.shape} | Test {X_test.shape}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1) Classical models (RF, XGB)\n",
    "# -------------------------------------------------------\n",
    "rf = RandomForestRegressor(n_estimators=300, n_jobs=-1, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\", n_estimators=600, learning_rate=0.05,\n",
    "    max_depth=6, subsample=0.8, colsample_bytree=0.8,\n",
    "    tree_method=\"hist\", n_jobs=-1, random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2) LSTM using the *same* features & same split\n",
    "# -------------------------------------------------------\n",
    "# Build full matrix in the same order so we can slice a test set that aligns in time\n",
    "X_full = pd.concat([X_train, X_test], axis=0)[feat_cols].to_numpy(dtype=np.float32)\n",
    "y_full = np.concatenate([y_train, y_test])\n",
    "cut = len(y_train)   # index where test begins\n",
    "\n",
    "SEQ_LEN = 30  # 30 two-minute bins ~ 1 hour, adjust if you like\n",
    "\n",
    "# Sliding-window sequences (target at the window's end)\n",
    "Xs, ys, tgt_idx = [], [], []\n",
    "for i in range(len(X_full) - SEQ_LEN):\n",
    "    Xs.append(X_full[i:i+SEQ_LEN])\n",
    "    ys.append(y_full[i+SEQ_LEN])\n",
    "    tgt_idx.append(i + SEQ_LEN)\n",
    "Xs, ys, tgt_idx = np.asarray(Xs, np.float32), np.asarray(ys, np.float32), np.asarray(tgt_idx)\n",
    "\n",
    "# Train/test split for sequences aligned by the same temporal cut\n",
    "mask_test = tgt_idx >= cut\n",
    "Xseq_tr, Yseq_tr = Xs[~mask_test], ys[~mask_test]\n",
    "Xseq_te, Yseq_te = Xs[mask_test],  ys[mask_test]\n",
    "\n",
    "# Normalize with TRAIN sequences only\n",
    "mu  = Xseq_tr.mean(axis=(0,1), keepdims=True)\n",
    "std = Xseq_tr.std(axis=(0,1), keepdims=True); std[std == 0] = 1e-6\n",
    "Xseq_tr = (Xseq_tr - mu) / std\n",
    "Xseq_te = (Xseq_te - mu) / std\n",
    "\n",
    "# LSTM model\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64, layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(n_features, hidden, num_layers=layers, batch_first=True, dropout=0.2)\n",
    "        self.fc   = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :]).squeeze(-1)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = LSTMRegressor(n_features=Xseq_tr.shape[2]).to(device)\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "lossf = nn.MSELoss()\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(Xseq_tr), torch.tensor(Yseq_tr))\n",
    "train_ld = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "\n",
    "epoch_losses = []\n",
    "model.train()\n",
    "for ep in range(6):\n",
    "    total = 0.0\n",
    "    for xb, yb in train_ld:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = lossf(pred, yb)\n",
    "        loss.backward(); opt.step()\n",
    "        total += loss.item()\n",
    "    epoch_losses.append(total/len(train_ld))\n",
    "    print(f\"Epoch {ep+1}: loss={epoch_losses[-1]:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_lstm = model(torch.tensor(Xseq_te, device=device)).cpu().numpy()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3) Metrics & comparison table\n",
    "# -------------------------------------------------------\n",
    "def r2_score_(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    ss_tot = np.sum((y_true - y_true.mean())**2) + 1e-12\n",
    "    return 1.0 - ss_res/ss_tot\n",
    "\n",
    "def smape(y_true, y_pred, eps=1e-6):\n",
    "    return 100.0*np.mean(np.abs(y_pred - y_true) / (np.abs(y_true)+np.abs(y_pred)+eps))\n",
    "\n",
    "def within_tol(y_true, y_pred, tol):\n",
    "    return 100.0*np.mean(np.abs(y_pred - y_true) <= tol)\n",
    "\n",
    "def row(name, yt, yp):\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"MAE (min)\": mean_absolute_error(yt, yp),\n",
    "        \"RMSE (min)\": mean_squared_error(yt, yp, squared=False),\n",
    "        \"R²\": r2_score_(yt, yp),\n",
    "        \"Bias (min)\": float(np.mean(yp - yt)),\n",
    "        \"sMAPE (%)\": smape(yt, yp),\n",
    "        \"≤5 min %\": within_tol(yt, yp, 5),\n",
    "        \"≤10 min %\": within_tol(yt, yp, 10),\n",
    "        \"N\": len(yt)\n",
    "    }\n",
    "\n",
    "# Note: LSTM test horizon equals the classical test horizon (aligned by `cut`)\n",
    "results = [\n",
    "    row(\"RandomForest\", y_test, y_pred_rf),\n",
    "    row(\"XGBoost\",      y_test, y_pred_xgb),\n",
    "    row(\"LSTM (seq)\",   Yseq_te, y_pred_lstm)\n",
    "]\n",
    "metrics_df = pd.DataFrame(results).sort_values(\"RMSE (min)\").reset_index(drop=True)\n",
    "display(metrics_df.style.format({\n",
    "    \"MAE (min)\": \"{:.2f}\", \"RMSE (min)\": \"{:.2f}\", \"R²\": \"{:.3f}\",\n",
    "    \"Bias (min)\": \"{:.2f}\", \"sMAPE (%)\": \"{:.1f}\", \"≤5 min %\": \"{:.1f}\", \"≤10 min %\": \"{:.1f}\"\n",
    "}))\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4) Visual comparisons\n",
    "# -------------------------------------------------------\n",
    "pairs = [\n",
    "    (\"RandomForest\", y_test, y_pred_rf),\n",
    "    (\"XGBoost\",      y_test, y_pred_xgb),\n",
    "    (\"LSTM (seq)\",   Yseq_te, y_pred_lstm)\n",
    "]\n",
    "\n",
    "# Common axis limits for Pred vs Actual\n",
    "all_true = np.concatenate([p[1] for p in pairs])\n",
    "a1, a2 = np.percentile(all_true, [1, 99])\n",
    "pad = 0.05*(a2-a1+1e-6)\n",
    "xy_min, xy_max = a1-pad, a2+pad\n",
    "\n",
    "# --- Figure A: Predicted vs Actual (per model)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 5))\n",
    "for ax, (name, yt, yp) in zip(axes, pairs):\n",
    "    n = len(yt); idx = np.linspace(0, n-1, min(n, 500)).astype(int)\n",
    "    ax.scatter(yt[idx], yp[idx], s=14, alpha=0.5, edgecolors=\"none\")\n",
    "    ax.plot([xy_min, xy_max], [xy_min, xy_max], \"k--\", lw=1)\n",
    "    ax.set_title(f\"{name} — Pred vs Actual\")\n",
    "    ax.set_xlabel(\"Actual RUL (min)\"); ax.set_ylabel(\"Predicted RUL (min)\")\n",
    "    ax.set_xlim(xy_min, xy_max); ax.set_ylim(xy_min, xy_max)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# --- Figure B: Error hist overlay + bias lines\n",
    "plt.figure(figsize=(9,5))\n",
    "bins = 70\n",
    "all_err = []\n",
    "for name, yt, yp in pairs:\n",
    "    err = yp - yt\n",
    "    all_err.append(err)\n",
    "    plt.hist(err, bins=bins, alpha=0.45, label=name)\n",
    "plt.axvline(0, color=\"k\", lw=1)\n",
    "for (name, yt, yp), err in zip(pairs, all_err):\n",
    "    mu = float(np.mean(err))\n",
    "    plt.axvline(mu, ls=\"--\", lw=1, label=f\"{name} bias={mu:.1f}\")\n",
    "plt.title(\"Prediction Error (Pred - Actual)\"); plt.xlabel(\"Error (min)\"); plt.ylabel(\"Count\")\n",
    "plt.grid(True, alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# --- Figure C: Accuracy vs tolerance curve\n",
    "plt.figure(figsize=(8,5))\n",
    "tols = np.arange(0, 61, 2)\n",
    "for name, yt, yp in pairs:\n",
    "    acc = [within_tol(yt, yp, t) for t in tols]\n",
    "    plt.plot(tols, acc, label=name)\n",
    "plt.xlabel(\"Tolerance (± minutes)\"); plt.ylabel(\"Within tolerance (%)\")\n",
    "plt.title(\"Accuracy vs Tolerance (higher is better)\")\n",
    "plt.grid(True, alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# --- Figure D: Residuals vs Actual (bias pattern check)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17,5), sharey=True)\n",
    "for ax, (name, yt, yp) in zip(axes, pairs):\n",
    "    res = yp - yt\n",
    "    n = len(yt); idx = np.linspace(0, n-1, min(n, 500)).astype(int)\n",
    "    ax.scatter(yt[idx], res[idx], s=12, alpha=0.5, edgecolors=\"none\")\n",
    "    ax.axhline(0, color=\"k\", lw=1)\n",
    "    ax.set_title(f\"{name} — Residuals vs Actual\")\n",
    "    ax.set_xlabel(\"Actual RUL (min)\"); ax.set_ylabel(\"Residual (min)\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# --- Figure E: MAE by actual-RUL bins\n",
    "def binned_mae(yt, yp, edges):\n",
    "    idx = np.digitize(yt, edges) - 1\n",
    "    out = []\n",
    "    for b in range(len(edges)-1):\n",
    "        m = (idx == b)\n",
    "        out.append(np.mean(np.abs(yp[m]-yt[m])) if np.any(m) else np.nan)\n",
    "    return np.array(out)\n",
    "\n",
    "edges = np.array([0,5,10,20,40,80,160,1e9])\n",
    "xt = [\"0–5\",\"5–10\",\"10–20\",\"20–40\",\"40–80\",\"80–160\",\">160\"]\n",
    "width = 0.25\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "for i, (name, yt, yp) in enumerate(pairs):\n",
    "    mae_bins = binned_mae(yt, yp, edges)\n",
    "    ax.bar(np.arange(len(xt))+i*width, mae_bins, width=width, label=name)\n",
    "ax.set_xticks(np.arange(len(xt))+width); ax.set_xticklabels(xt)\n",
    "ax.set_ylabel(\"MAE (min)\"); ax.set_title(\"MAE by Actual-RUL Bin (lower is better)\")\n",
    "ax.grid(True, axis=\"y\", alpha=0.3); ax.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# --- Figure F: First N time steps (per model)\n",
    "N = 220\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 8), sharex=True)\n",
    "for ax, (name, yt, yp) in zip(axes, pairs):\n",
    "    ax.plot(yt[:N], lw=1.2, label=\"Actual\")\n",
    "    ax.plot(yp[:N], lw=1.2, label=\"Pred\")\n",
    "    ax.set_title(f\"{name} — First {N} samples\"); ax.set_ylabel(\"RUL (min)\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "axes[-1].set_xlabel(\"Sample index\"); axes[0].legend(loc=\"upper right\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# --- (Optional) LSTM epoch losses\n",
    "plt.figure(figsize=(6.5,3.5))\n",
    "plt.plot(range(1,len(epoch_losses)+1), epoch_losses, marker=\"o\")\n",
    "plt.title(\"LSTM training loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"MSE\")\n",
    "plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
